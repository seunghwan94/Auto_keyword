
import streamlit as st
import pandas as pd
import os
import json

from login import login_page
from crawler.blog_crawler import BlogCrawler
from crawler.naver_keywords import get_naver_related_keywords
from crawler.google_keywords import get_google_autocomplete
from keyword_stat.naver_api import NaverAPI
from analyzer.noun_extractor import extract_nouns
from analyzer.stopword_cleaner import load_stopwords, remove_stopwords
from analyzer.keyword_ranker import rank_keywords
import pandas as pd

CONFIG_PATH = "config/deluxe_config.json"

def load_naver_config():
    try:
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            config = json.load(f)
            return config["client_id"], config["client_secret"], config["customer_id"]
    except Exception as e:
        print(f"[ERROR] ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None, None

def get_keyword_stat(keyword):
    client_id, client_secret, customer_id = load_naver_config()
    if not all([client_id, client_secret, customer_id]):
        return None

    # ì‹¤ì œ API í‚¤ëŠ” ë³„ë„ë¡œ ì•ˆì „í•˜ê²Œ ë¡œë“œí•´ì•¼ í•¨ (ì˜ˆì‹œ ê°’ ì‚¬ìš©)
    API_KEY = client_id
    SECRET_KEY = client_secret
    CUSTOMER_ID = customer_id

    api = NaverAPI(client_id, client_secret, API_KEY, SECRET_KEY, CUSTOMER_ID)
    df = api.get_keyword_data(keyword)

    if df.empty:
        return None

    stat_row = df[df["relKeyword"] == keyword]
    if stat_row.empty:
        stat_row = df.iloc[[0]]  # ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ê²°ê³¼ë¼ë„ ë°˜í™˜

    return stat_row.iloc[0].to_dict()




st.set_page_config(page_title="SEO í‚¤ì›Œë“œ ë¶„ì„ê¸°", layout="wide")

# ë¡œê·¸ì¸ ì²˜ë¦¬
if "authenticated" not in st.session_state:
    login_page()
    st.stop()

# ë“±ê¸‰ í”Œëœ í‘œì‹œ
plan_grade = st.session_state.get("user_grade", 1)
plan_name = {1: "ğŸŸ¢ STANDARD", 2: "ğŸ”µ DELUXE", 3: "ğŸŸ£ PREMIUM"}.get(plan_grade, "ğŸŸ¢ STANDARD")
st.sidebar.success(f"ë¡œê·¸ì¸ë¨: {st.session_state['user_email']}\në“±ê¸‰: {plan_name}")

# DELUXE í”Œëœ ì…ë ¥ UI
if plan_grade in [2, 3]:  # DELUXEì™€ PREMIUM í”Œëœ ëª¨ë‘ ê°€ëŠ¥
    st.sidebar.title("ğŸ”‘ DELUXE í”Œëœ ì„¤ì •")
    client_id = st.sidebar.text_input("ë„¤ì´ë²„ Client ID")
    client_secret = st.sidebar.text_input("ë„¤ì´ë²„ Client Secret", type="password")
    customer_id = st.sidebar.text_input("ë„¤ì´ë²„ Customer ID")
    
    if st.sidebar.button("ì €ì¥"):
        config_data = {
            "client_id": client_id,
            "client_secret": client_secret,
            "customer_id": customer_id
        }
        with open("config/deluxe_config.json", "w", encoding="utf-8") as f:
            json.dump(config_data, f, ensure_ascii=False, indent=2)
        st.sidebar.success("âœ… ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# PREMIUM í”Œëœ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ì„¤ì •
if plan_grade == 3:
    st.sidebar.title("ğŸ“¥ PREMIUM í”Œëœ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ì„¤ì •")
    if st.sidebar.button("ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œ"):
        # ì—‘ì…€ íŒŒì¼ë¡œ ë­í‚¹ ì €ì¥
        df_keywords = pd.DataFrame(ranked_keywords, columns=["í‚¤ì›Œë“œ", "ë¹ˆë„ìˆ˜"])
        file_path = "ranked_keywords.xlsx"
        df_keywords.to_excel(file_path, index=False)
        st.sidebar.success(f"âœ… ì—‘ì…€ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {file_path}")
        st.download_button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", file_path)

# ë¶„ì„ í‚¤ì›Œë“œ ì…ë ¥
st.title(f"{plan_name} í‚¤ì›Œë“œ ë¶„ì„ê¸°")
keyword = st.text_input("ë¶„ì„í•  ì£¼ì œ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë‹¤ì´ì–´íŠ¸ ì‹ë‹¨):", "")

if keyword:
    st.info(f"'{keyword}' ê´€ë ¨ ë¸”ë¡œê·¸ ë³¸ë¬¸ê³¼ ì—°ê´€ê²€ìƒ‰ì–´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

    with st.spinner("ğŸ“¦ ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
        crawler = BlogCrawler()
        naver_posts = crawler.crawl_naver_blogs(keyword, max_posts=5)
        tistory_posts = crawler.crawl_tistory_blogs(keyword, max_posts=5)
        all_posts = [p for p in (naver_posts + tistory_posts) if p]
        full_text = "\n".join([p["content"] for p in all_posts])

        related_naver = get_naver_related_keywords(keyword)
        related_google = get_google_autocomplete(keyword)
        related_text = " ".join(related_naver + related_google)

    total_text = full_text + " " + related_text
    nouns = extract_nouns(total_text)
    stopwords = load_stopwords("assets/stopwords.txt")
    filtered = remove_stopwords(nouns, stopwords)
    ranked_keywords = rank_keywords(filtered, top_n=30)

    # ğŸ“Š í‚¤ì›Œë“œ ë­í‚¹
    df_keywords = pd.DataFrame(ranked_keywords, columns=["í‚¤ì›Œë“œ", "ë¹ˆë„ìˆ˜"])
    st.subheader("ğŸ“Š í‚¤ì›Œë“œ ë­í‚¹ TOP 30")
    st.table(df_keywords)

    # DELUXE ì´ìƒì¼ ë•Œ ê²€ìƒ‰ëŸ‰ ì¶œë ¥
    if plan_grade >= 2:
        get_keyword_stat(get_keyword_stat)
    else:
        st.warning("ğŸ”’ Deluxe í”Œëœë¶€í„° ê²€ìƒ‰ëŸ‰/ê²½ìŸë„ ë¶„ì„ ê¸°ëŠ¥ì„ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # PREMIUM ì „ìš© ê¸°ëŠ¥ (ì—‘ì…€ ë‹¤ìš´ë¡œë“œ)
    if plan_grade >= 3:
        st.success("ğŸ“¥ í”„ë¦¬ë¯¸ì—„ ì‚¬ìš©ì: ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    with st.expander("ğŸ§© ì—°ê´€ê²€ìƒ‰ì–´"):
        st.markdown("**ë„¤ì´ë²„:**")
        st.write(", ".join(related_naver))
        st.markdown("**êµ¬ê¸€:**")
        st.write(", ".join(related_google))

    with st.expander("ğŸ“° ë¸”ë¡œê·¸ ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°"):
        for post in all_posts:
            st.markdown(f"**[{post['source'].upper()}] {post['title']}**")
            st.caption(post["url"])
            st.write(post["content"][:300] + "...")
else:
    st.warning("ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")




from collections import Counter
import pandas as pd
from konlpy.tag import Okt
from utils.other_path import resolve_path

def extract_nouns(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œ
    """
    okt = Okt()
    nouns = okt.nouns(text)
    return [noun for noun in nouns if len(noun) > 1]


def rank_keywords(words, top_n=20):
    """
    ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¹ˆë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ TOP N ë°˜í™˜
    """
    counter = Counter(words)
    return counter.most_common(top_n)


def get_recommended_keywords(ranked_df, stat_df, top_n=10, naver=[], google=[]):
    keywords_ranked = list(ranked_df["í‚¤ì›Œë“œ"])
    keywords_naver = naver
    keywords_google = google
    keywords_all = keywords_ranked + keywords_naver + keywords_google

    stat_df["ì´ ê²€ìƒ‰ëŸ‰"] = stat_df["ì›”ê°„ ê²€ìƒ‰ìˆ˜_PC"] + stat_df["ì›”ê°„ ê²€ìƒ‰ìˆ˜_ëª¨ë°”ì¼"]
    stat_df["ê²½ìŸ ì§€ìˆ˜"] = stat_df["ê²½ìŸ ì§€ìˆ˜"].fillna("ì—†ìŒ")

    recommended = pd.DataFrame(columns=["ì—°ê´€ í‚¤ì›Œë“œ", "ì´ ê²€ìƒ‰ëŸ‰", "ê²½ìŸ ì§€ìˆ˜"])

    def add_candidates(keywords, already_added, max_count):
        candidates = stat_df[
            (stat_df["ì—°ê´€ í‚¤ì›Œë“œ"].isin(keywords)) &
            (~stat_df["ì—°ê´€ í‚¤ì›Œë“œ"].isin(already_added))
        ]
        candidates = candidates.sort_values(by="ì´ ê²€ìƒ‰ëŸ‰", ascending=False)
        return candidates[["ì—°ê´€ í‚¤ì›Œë“œ", "ì´ ê²€ìƒ‰ëŸ‰", "ê²½ìŸ ì§€ìˆ˜"]].head(max_count)

    # ğŸ¥‡ 1ë‹¨ê³„: ranked_df + naver + google êµì§‘í•©
    intersection_keywords = set(keywords_ranked) & set(keywords_naver + keywords_google)
    step1 = add_candidates(intersection_keywords, set(), top_n)
    recommended = pd.concat([recommended, step1], ignore_index=True)

    # ğŸ¥ˆ 2ë‹¨ê³„: ranked + (naver or google) ì¶”ê°€
    remaining = top_n - len(recommended)
    if remaining > 0:
        step2_keywords = set(keywords_ranked) | set(keywords_naver) | set(keywords_google)
        step2 = add_candidates(step2_keywords, set(recommended["ì—°ê´€ í‚¤ì›Œë“œ"]), remaining)
        recommended = pd.concat([recommended, step2], ignore_index=True)

    # ğŸ¥‰ 3ë‹¨ê³„: ranked_dfë§Œ ì¶”ê°€
    remaining = top_n - len(recommended)
    if remaining > 0:
        step3 = add_candidates(keywords_ranked, set(recommended["ì—°ê´€ í‚¤ì›Œë“œ"]), remaining)
        recommended = pd.concat([recommended, step3], ignore_index=True)

    # ğŸ… 4ë‹¨ê³„: ê²½ìŸ ì§€ìˆ˜ ì¤‘
    remaining = top_n - len(recommended)
    if remaining > 0:
        step4 = stat_df[
            (stat_df["ê²½ìŸ ì§€ìˆ˜"].isin(["ì¤‘", "ì¤‘ê°„"])) &
            (~stat_df["ì—°ê´€ í‚¤ì›Œë“œ"].isin(set(recommended["ì—°ê´€ í‚¤ì›Œë“œ"])))
        ].sort_values(by="ì´ ê²€ìƒ‰ëŸ‰", ascending=False)
        step4 = step4[["ì—°ê´€ í‚¤ì›Œë“œ", "ì´ ê²€ìƒ‰ëŸ‰", "ê²½ìŸ ì§€ìˆ˜"]].head(remaining)
        recommended = pd.concat([recommended, step4], ignore_index=True)

    return recommended.drop_duplicates(subset=["ì—°ê´€ í‚¤ì›Œë“œ"]).head(top_n)


def load_stopwords(path="utils/config/stopwords.txt"):
    with open(resolve_path(path), "r", encoding="utf-8") as f:
        return set(line.strip() for line in f if line.strip())

def remove_stopwords(words, stopwords):
    """
    ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ í•„í„°ë§
    """
    return [word for word in words if word not in stopwords]
